{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d20282-bdb6-4bb0-ad20-7635b700beb5",
   "metadata": {},
   "source": [
    "# Task specific knowledge distillation for BERT using `HuggingFace/transformers` and `IntelLabs/Model-Compression-Research-Package`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40903b88-ff89-436e-9096-3bcb00d95b09",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6c742-f726-47ce-8a72-9a060c107718",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -H /venv/bin/pip install -U transformers datasets tensorboard\n",
    "! (git clone https://github.com/IntelLabs/Model-Compression-Research-Package.git || :) && sudo -H /venv/bin/pip install ./Model-Compression-Research-Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ae010-afc5-4c66-a397-460e060d6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "import model_compression_research as mcr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d878537-d5dc-44ea-8cc4-c91bb220a6c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this example we will use the following models from the HuggingFace models hub. It is important to make sure both models use tokenizer since the student's tokenizer will be used to process the input for both models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4780360-20e4-419c-9fe9-76c0d4f1bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "teacher_id = \"textattack/bert-base-uncased-SST-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712ae77-8ffe-4517-8b62-8857dafe14a0",
   "metadata": {},
   "source": [
    "## Datasets & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d004b-e932-400e-956d-ff35de23de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"glue\"\n",
    "dataset_config = \"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cca702-450b-4d0b-80f0-90a65f4aad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dataset_id, dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee82266-ebdd-4fae-af3c-a8cb64f0c45c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09770c-520e-43b3-8058-63b6c0e39244",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(student_id)\n",
    "teacher_tokenizer = transformers.AutoTokenizer.from_pretrained(teacher_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aadba2-d125-4b74-ba54-6858ea5bb10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = min(tokenizer.model_max_length, teacher_tokenizer.model_max_length)\n",
    "def process(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, max_length=max_seq_len\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(process, batched=True)\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "\n",
    "# tokenized_datasets[\"test\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bc578-9168-43d8-af4f-8af96d42de79",
   "metadata": {},
   "source": [
    "## Setup training and distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21357326-3820-4b59-8d7f-4ebc3c0c023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label2id, id2label dicts for nice outputs for the model\n",
    "labels = tokenized_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./run',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    fp16=True,\n",
    "    learning_rate=6e-5,\n",
    "    seed=33,\n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"epoch\", # to get more information to TB\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "data_collator  = transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8 if training_args.fp16 else 1)\n",
    "\n",
    "student_model = transformers.AutoModelForSequenceClassification.from_pretrained(student_id, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "teacher_model = transformers.AutoModelForSequenceClassification.from_pretrained(teacher_id, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "distillation_model = mcr.hf_add_teacher_to_student(\n",
    "    student_model,\n",
    "    teacher_model,\n",
    "    # scaling factor for CE loss of the student's logits against the ground truth labels\n",
    "    student_alpha=0.5,\n",
    "    # scaling factor for CE loss of the student's against the teacher's logits (KL divergence)\n",
    "    teacher_ce_alpha=0.5,\n",
    "    # temperature applied to the CE loss of the student's against the teacher's logits\n",
    "    teacher_ce_temperature=4.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c91b09-d4cd-410b-b3be-b7210d2a5a93",
   "metadata": {},
   "source": [
    "## Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25214c-145f-4ef2-8312-7c0aa556423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics and metrics function\n",
    "accuracy_metric = datasets.load_metric( \"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dcec2-5730-437a-999c-ce2b09fd0db2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c06963-bb92-4913-b644-de055017be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=distillation_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556fd60-c46d-459f-8bc7-6797bae91b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9face93-c7d9-45c7-8cb1-cde8dc0ea48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final trained model by removing the teacher from it\n",
    "final_model = mcr.hf_remove_teacher_from_student(distillation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6df20-898a-44d5-925b-d1dfa3377d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
